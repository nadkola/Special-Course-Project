"""
app.py â€” Business Intelligence OLAP Assistant (Tier 2)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
A Streamlit web application that lets business users perform OLAP analysis
through natural-language conversation, powered by an LLM (Anthropic Claude
or OpenAI GPT).

Architecture:
  User question  â†’  LLM (with schema context)  â†’  pandas code + narrative
  pandas code    â†’  safe execution on cached DataFrame  â†’  table / chart
"""

import streamlit as st
import pandas as pd
import numpy as np
import traceback
import re

from data_utils import load_data, get_schema_description, get_dataset_overview
from prompts import build_system_prompt, build_user_message, parse_llm_response


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PAGE CONFIG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.set_page_config(
    page_title="OLAP Assistant â€” BI Analytics",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CUSTOM CSS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.markdown("""
<style>
    /* Metric cards */
    div[data-testid="stMetric"] {
        background: linear-gradient(135deg, #667eea11, #764ba211);
        border: 1px solid #e0e0e0;
        border-radius: 10px;
        padding: 12px 16px;
    }
    /* Chat messages */
    .stChatMessage { border-radius: 12px; }
    /* Sidebar header */
    [data-testid="stSidebarContent"] h1 { font-size: 1.3rem; }
    /* Result tables */
    .stDataFrame { border-radius: 8px; overflow: hidden; }
</style>
""", unsafe_allow_html=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD DATA (cached)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@st.cache_data
def cached_load():
    return load_data()

df = cached_load()
schema_text = get_schema_description(df)
overview = get_dataset_overview(df)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SIDEBAR â€” Configuration & Dataset Overview
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with st.sidebar:
    st.title("ğŸ“Š OLAP Assistant")
    st.caption("Business Intelligence through Natural Language")

    # --- LLM Provider Selection ---
    st.subheader("âš™ï¸ LLM Configuration")
    provider = st.selectbox("Provider", ["Anthropic (Claude)", "OpenAI (GPT)"], index=0)

    # Auto-load API key from secrets.toml, fall back to manual input
    saved_key = ""
    try:
        if "Anthropic" in provider and "ANTHROPIC_API_KEY" in st.secrets:
            saved_key = st.secrets["ANTHROPIC_API_KEY"]
        elif "OpenAI" in provider and "OPENAI_API_KEY" in st.secrets:
            saved_key = st.secrets["OPENAI_API_KEY"]
    except Exception:
        pass

    if saved_key:
        api_key = saved_key
        st.success("âœ… API key loaded from secrets")
    else:
        api_key = st.text_input(
            "API Key",
            type="password",
            help="Enter your API key here, or save it in .streamlit/secrets.toml to auto-load.",
        )

    # Model selection
    if "Anthropic" in provider:
        model = st.selectbox("Model", [
            "claude-sonnet-4-20250514",
            "claude-haiku-4-5-20250929",
        ], index=0)
    else:
        model = st.selectbox("Model", [
            "gpt-4o",
            "gpt-4o-mini",
        ], index=0)

    st.divider()

    # --- Dataset Overview ---
    st.subheader("ğŸ“‹ Dataset Overview")
    st.markdown(f"**Records:** {overview['total_rows']:,}")
    st.markdown(f"**Period:** {overview['date_range']}")
    st.markdown(f"**Revenue:** ${overview['total_revenue']:,.2f}")
    st.markdown(f"**Profit:** ${overview['total_profit']:,.2f}")
    st.markdown(f"**Avg Margin:** {overview['avg_profit_margin']:.1f}%")

    with st.expander("Dimensions"):
        st.markdown(f"**Regions ({len(overview['regions'])}):** {', '.join(overview['regions'])}")
        st.markdown(f"**Categories ({len(overview['categories'])}):** {', '.join(overview['categories'])}")
        st.markdown(f"**Segments ({len(overview['segments'])}):** {', '.join(overview['segments'])}")
        st.markdown(f"**Countries:** {overview['countries_count']}")
        st.markdown(f"**Years:** {', '.join(str(y) for y in overview['years'])}")

    with st.expander("Hierarchies"):
        st.markdown("ğŸ• **Time:** Year â†’ Quarter â†’ Month")
        st.markdown("ğŸŒ **Geography:** Region â†’ Country")
        st.markdown("ğŸ“¦ **Product:** Category â†’ Subcategory")
        st.markdown("ğŸ‘¤ **Customer:** Segment")

    st.divider()

    # --- Quick Actions ---
    st.subheader("âš¡ Quick Queries")
    quick_queries = {
        "Revenue by Region": "Show total revenue by region",
        "2025 by Quarter": "Break down 2025 revenue by quarter",
        "Electronics in Europe": "Show Electronics sales in Europe by country",
        "2024 vs 2025": "Compare 2024 vs 2025 total revenue by region",
        "3-Year Trend": "Show yearly revenue trend for 2023, 2024, and 2025",
        "Top 5 Countries": "What are the top 5 countries by profit?",
        "Monthly Trend 2025": "Show monthly revenue trend for 2025",
        "Category Margins": "Which category has the highest profit margin?",
    }
    for label, query in quick_queries.items():
        if st.button(f"â–¶ {label}", use_container_width=True, key=f"quick_{label}"):
            st.session_state["pending_query"] = query

    st.divider()
    if st.button("ğŸ—‘ï¸ Clear Chat History", use_container_width=True):
        st.session_state["messages"] = []
        st.rerun()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LLM INTEGRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def call_llm(user_query: str, conversation_history: list) -> str:
    """
    Send the user's question to the configured LLM and return the raw response.
    Supports Anthropic Claude and OpenAI GPT.
    """
    system_prompt = build_system_prompt(schema_text)

    if "Anthropic" in provider:
        try:
            from anthropic import Anthropic
        except ImportError:
            st.error("Please install the Anthropic SDK: `pip install anthropic`")
            return ""

        client = Anthropic(api_key=api_key)

        # Build messages from conversation history
        messages = []
        for msg in conversation_history:
            messages.append({"role": msg["role"], "content": msg["content"]})

        response = client.messages.create(
            model=model,
            max_tokens=2048,
            system=system_prompt,
            messages=messages,
        )
        return response.content[0].text

    else:  # OpenAI
        try:
            from openai import OpenAI
        except ImportError:
            st.error("Please install the OpenAI SDK: `pip install openai`")
            return ""

        client = OpenAI(api_key=api_key)

        messages = [{"role": "system", "content": system_prompt}]
        for msg in conversation_history:
            messages.append({"role": msg["role"], "content": msg["content"]})

        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=2048,
            temperature=0.1,  # Low temperature for analytical consistency
        )
        return response.choices[0].message.content


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SAFE CODE EXECUTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def execute_pandas_code(code: str, dataframe: pd.DataFrame) -> pd.DataFrame | None:
    """
    Safely execute LLM-generated pandas code against the DataFrame.

    Security measures:
      â€¢ Code runs in a restricted namespace (only pd, np, df).
      â€¢ Dangerous builtins (exec, eval, open, __import__) are blocked.
      â€¢ Output must be a DataFrame assigned to `result`.
    """
    # Basic security checks
    dangerous_patterns = [
        r"\bimport\b", r"\bexec\b", r"\beval\b", r"\bopen\b",
        r"\b__import__\b", r"\bos\.", r"\bsys\.", r"\bsubprocess\b",
        r"\bshutil\b", r"\bpathlib\b",
    ]
    for pattern in dangerous_patterns:
        if re.search(pattern, code):
            # Allow 'import' only if it's inside a comment or part of pandas syntax
            if pattern == r"\bimport\b":
                # Remove comments and string literals, then check
                cleaned = re.sub(r"#.*$", "", code, flags=re.MULTILINE)
                cleaned = re.sub(r"'[^']*'|\"[^\"]*\"", "", cleaned)
                if re.search(pattern, cleaned):
                    raise SecurityError(f"Blocked potentially unsafe operation: {pattern}")
            else:
                raise SecurityError(f"Blocked potentially unsafe operation: {pattern}")

    # Prepare execution namespace with safe builtins
    safe_builtins = {
        "round": round,
        "len": len,
        "min": min,
        "max": max,
        "sum": sum,
        "abs": abs,
        "sorted": sorted,
        "range": range,
        "list": list,
        "dict": dict,
        "tuple": tuple,
        "set": set,
        "str": str,
        "int": int,
        "float": float,
        "bool": bool,
        "enumerate": enumerate,
        "zip": zip,
        "map": map,
        "filter": filter,
        "isinstance": isinstance,
        "type": type,
        "print": print,
        "True": True,
        "False": False,
        "None": None,
    }

    namespace = {
        "df": dataframe.copy(),
        "pd": pd,
        "np": np,
        "result": None,
    }

    exec(code, {"__builtins__": safe_builtins}, namespace)

    result = namespace.get("result")
    if result is None:
        raise ValueError("Code did not produce a `result` variable.")
    if not isinstance(result, pd.DataFrame):
        # Try to convert Series to DataFrame
        if isinstance(result, pd.Series):
            result = result.to_frame().reset_index()
        else:
            raise TypeError(f"Expected DataFrame, got {type(result).__name__}")
    return result


class SecurityError(Exception):
    """Raised when LLM-generated code contains unsafe operations."""
    pass


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUTO-CHART LOGIC
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def auto_chart(result_df: pd.DataFrame):
    """
    Automatically select and render an appropriate chart based on the
    shape of the result DataFrame.
    """
    if result_df is None or result_df.empty or len(result_df) < 2:
        return

    # Identify numeric and text columns
    num_cols = result_df.select_dtypes(include=[np.number]).columns.tolist()
    text_cols = result_df.select_dtypes(include=["object", "category"]).columns.tolist()

    if not num_cols or not text_cols:
        return

    label_col = text_cols[0]
    # Pick the most meaningful numeric column (prefer 'revenue', 'profit', etc.)
    priority = ["revenue", "profit", "quantity", "orders", "profit_margin"]
    value_col = None
    for p in priority:
        matches = [c for c in num_cols if p in c.lower()]
        if matches:
            value_col = matches[0]
            break
    if value_col is None:
        value_col = num_cols[0]

    chart_df = result_df[[label_col, value_col]].copy()
    chart_df = chart_df.set_index(label_col)

    # Decide chart type
    n_rows = len(chart_df)
    is_time_series = label_col.lower() in [
        "month", "month_name", "quarter", "year", "order_date",
    ]

    if is_time_series and n_rows <= 24:
        st.line_chart(chart_df, use_container_width=True)
    elif n_rows <= 15:
        st.bar_chart(chart_df, use_container_width=True)
    # Skip chart for very large result sets


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN INTERFACE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.title("ğŸ“Š Business Intelligence OLAP Assistant")
st.markdown(
    "Ask questions about the **Global Retail Sales** dataset in plain English. "
    "The assistant translates your questions into OLAP operations and returns "
    "formatted results with insights."
)

# --- Initialization ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# --- Display Overview Metrics ---
col1, col2, col3, col4 = st.columns(4)
col1.metric("Total Revenue", f"${overview['total_revenue']:,.0f}")
col2.metric("Total Profit", f"${overview['total_profit']:,.0f}")
col3.metric("Avg Margin", f"{overview['avg_profit_margin']:.1f}%")
col4.metric("Transactions", f"{overview['total_rows']:,}")

st.divider()

# --- Chat History ---
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        if msg["role"] == "assistant":
            # Re-render assistant messages with their saved components
            if "analysis" in msg:
                st.markdown(msg["analysis"])
            if "result_df" in msg and msg["result_df"] is not None:
                with st.expander("ğŸ“‹ Data Table", expanded=True):
                    st.dataframe(msg["result_df"], use_container_width=True)
                auto_chart(msg["result_df"])
            if "code" in msg and msg["code"]:
                with st.expander("ğŸ”§ Generated Code"):
                    st.code(msg["code"], language="python")
            if "error" in msg:
                st.error(msg["error"])
        else:
            st.markdown(msg["content"])

# --- Handle pending quick queries ---
pending = st.session_state.pop("pending_query", None)

# --- Chat Input ---
user_input = st.chat_input("Ask a question about the dataâ€¦")
query = pending or user_input

if query:
    # Display user message
    with st.chat_message("user"):
        st.markdown(query)
    st.session_state["messages"].append({"role": "user", "content": query})

    # Validate API key
    if not api_key:
        with st.chat_message("assistant"):
            err = "âš ï¸ Please enter your API key in the sidebar to get started."
            st.warning(err)
        st.session_state["messages"].append({
            "role": "assistant", "content": err, "analysis": err,
        })
    else:
        # Call LLM
        with st.chat_message("assistant"):
            with st.spinner("Analyzing your questionâ€¦"):
                try:
                    # Build conversation for context
                    conv_for_llm = []
                    # Include last 6 messages for context (3 exchanges)
                    recent = st.session_state["messages"][-7:]  # includes current user msg
                    for m in recent:
                        conv_for_llm.append({
                            "role": m["role"],
                            "content": m["content"],
                        })

                    raw_response = call_llm(query, conv_for_llm)
                    parsed = parse_llm_response(raw_response)

                    analysis = parsed["analysis"]
                    code = parsed["code"]
                    result_df = None
                    error_msg = None

                    # Display analysis
                    if analysis:
                        st.markdown(analysis)

                    # Execute code (with auto-retry on failure)
                    max_retries = 2
                    attempt = 0

                    while code and attempt < max_retries:
                        attempt += 1
                        try:
                            result_df = execute_pandas_code(code, df)

                            # Check if result is empty
                            if result_df is not None and not result_df.empty:
                                break  # Success â€” exit retry loop
                            else:
                                # Empty result â€” ask LLM to fix
                                if attempt < max_retries:
                                    retry_msg = (
                                        f"Your code executed but returned an EMPTY DataFrame (0 rows). "
                                        f"This means your filters matched nothing. "
                                        f"Here are the EXACT values available in the dataset:\n"
                                        f"  years: {sorted(df['year'].unique().tolist())}\n"
                                        f"  quarters: {sorted(df['quarter'].unique().tolist())}\n"
                                        f"  regions: {sorted(df['region'].unique().tolist())}\n"
                                        f"  categories: {sorted(df['category'].unique().tolist())}\n"
                                        f"  countries: {sorted(df['country'].unique().tolist())}\n"
                                        f"  segments: {sorted(df['customer_segment'].unique().tolist())}\n"
                                        f"Please fix your code to use only these exact values. "
                                        f"Original question: {query}"
                                    )
                                    retry_conv = conv_for_llm + [
                                        {"role": "assistant", "content": raw_response},
                                        {"role": "user", "content": retry_msg},
                                    ]
                                    raw_response = call_llm(retry_msg, retry_conv)
                                    parsed = parse_llm_response(raw_response)
                                    code = parsed["code"]
                                    if parsed["analysis"]:
                                        analysis = parsed["analysis"]
                                else:
                                    error_msg = "âš ï¸ The query returned no data after retrying. The filters may not match any records."

                        except SecurityError as se:
                            error_msg = f"ğŸ”’ Security: {se}"
                            break
                        except Exception as exec_err:
                            if attempt < max_retries:
                                # Send error to LLM for self-correction
                                retry_msg = (
                                    f"Your code raised an error: {exec_err}\n"
                                    f"The failing code was:\n```python\n{code}\n```\n"
                                    f"Please fix the code. Remember: use only pd, np, df. "
                                    f"Assign the result to `result`. "
                                    f"Original question: {query}"
                                )
                                retry_conv = conv_for_llm + [
                                    {"role": "assistant", "content": raw_response},
                                    {"role": "user", "content": retry_msg},
                                ]
                                raw_response = call_llm(retry_msg, retry_conv)
                                parsed = parse_llm_response(raw_response)
                                code = parsed["code"]
                                if parsed["analysis"]:
                                    analysis = parsed["analysis"]
                            else:
                                error_msg = f"âš ï¸ Code execution error: {exec_err}"

                    # Display results
                    if result_df is not None and not result_df.empty:
                        with st.expander("ğŸ“‹ Data Table", expanded=True):
                            st.dataframe(
                                result_df,
                                use_container_width=True,
                                hide_index=True,
                            )
                        auto_chart(result_df)
                    elif error_msg:
                        st.error(error_msg)
                        st.caption("Try rephrasing your question or check the Generated Code below.")

                    # Always show the generated code
                    if code:
                        with st.expander("ğŸ”§ Generated Code"):
                            st.code(code, language="python")

                    # Save assistant message
                    assistant_msg = {
                        "role": "assistant",
                        "content": analysis or raw_response,
                        "analysis": analysis,
                        "code": code,
                        "result_df": result_df if result_df is not None else None,
                    }
                    if error_msg:
                        assistant_msg["error"] = error_msg
                    st.session_state["messages"].append(assistant_msg)

                except Exception as e:
                    error_text = f"âŒ Error communicating with the LLM: {str(e)}"
                    st.error(error_text)
                    st.caption(
                        "Check that your API key is correct and the selected model is available."
                    )
                    st.session_state["messages"].append({
                        "role": "assistant",
                        "content": error_text,
                        "analysis": error_text,
                        "error": str(e),
                    })


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FOOTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.divider()
st.caption(
    "ğŸ’¡ **Tip:** Try questions like *'Total revenue by region'*, "
    "*'Compare 2023 vs 2024 vs 2025'*, or *'Drill down Q4 2025 by month'*. "
    "Use the Quick Queries in the sidebar to get started."
)
